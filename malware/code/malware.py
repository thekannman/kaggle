# -*- coding: utf-8 -*-
"""
Created on Mon Feb 09 16:50:14 2015

@author: VishnuC
@email: vrajs5@gmail.com
Beating the benchmark for Microsoft Malware Classification Challenge (BIG 2015)
"""
from sys import exit
import os
import numpy as np
import gzip
from csv import reader, writer
from sklearn.ensemble import RandomForestClassifier as RFClassifier, ExtraTreesClassifier as ETClassifier
import six
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.feature_selection import RFECV
from sklearn.cross_validation import StratifiedKFold
import matplotlib.pyplot as plt
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import log_loss
from sklearn.linear_model import Lasso,ElasticNet,Ridge,LinearRegression,LogisticRegression,SGDClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN

class RFClassifierWithCoef(RFClassifier):
    def fit(self, *args, **kwargs):
        super(RFClassifierWithCoef, self).fit(*args, **kwargs)
        self.coef_ = self.feature_importances_

class ETClassifierWithCoef(ETClassifier):
    def fit(self, *args, **kwargs):
        super(ETClassifierWithCoef, self).fit(*args, **kwargs)
        self.coef_ = self.feature_importances_

# Decide read/write mode based on python version
read_mode, write_mode = ('r','w') if six.PY2 else ('rt','wt')

# Decide zip based on python version
if six.PY2:
    from itertools import izip
    zp = izip
else:
    zp = zip

# Set path to your consolidated files
path = '/home/zak/kaggle/malware/'
os.chdir(path)

# File names
ftrain = 'train_consolidation.gz'
ftest = 'test_consolidation.gz'
flabel = 'trainLabels.csv'
fsubmission = 'submission.gz'

print('loading started')
# Lets read labels first as things are not sorted in files
labels = {}
with open(flabel) as f:
    next(f)    # Ignoring header
    for row in reader(f):
        labels[row[0]] = int(row[1])
print('labels loaded')

# Dimensions for train set
ntrain = 10868
max_features= 65796
ntwobyte = 256
nfeatures = 3 + ntwobyte  # 259 to not use four_byte
nfourbyte = 20536
npca = 0 #500
nrows = nfeatures + npca + 5 + ntwobyte 
train = np.zeros((ntrain, nrows), dtype = float)
feature_array = np.zeros(ntwobyte, dtype=float)
#train_fourbyte = np.zeros((ntrain, nfourbyte), dtype = int)
y_train = np.zeros(ntrain, dtype=int)
with gzip.open(ftrain, read_mode) as f:
    next(f)    # Ignoring header
    for t,row in enumerate(reader(f)):
        train[t,1:nfeatures+1] = map(float, row[1:nfeatures+1]) if six.PY2 else list(map(float, row[1:nfeatures+1]))
        feature_array = train[t,4:nfeatures+1]
        train[t,nfeatures+1] = np.median(feature_array)
        train[t,nfeatures+2] = np.std(feature_array)
        train[t,nfeatures+3] = np.max(feature_array)
	train[t,nfeatures+4] = np.min(feature_array)
        sort_list = sorted(range(len(feature_array)),key=lambda x:feature_array[x], reverse=True)
        train[t,nfeatures+5:nfeatures+5+ntwobyte] = sorted(range(len(feature_array)),key=lambda x:sort_list[x])
#	train_fourbyte[t] = map(float, row[nfeatures+1:nfeatures+nfourbyte+1]) if six.PY2 else list(map(float, row[nfeatures+1:nfeatures+nfourbyte+1]))
        y_train[t] = labels[row[0]]
        if(t+1)%1000==0:
            print(t+1, 'records loaded')

print('training set loaded')

del labels

# Parameters for Randomforest
pars2 = {'criterion':['gini','entropy'],'max_depth':[2,4,6,10,None],'min_samples_split':[2,4]}
#clf1 = RFClassifierWithCoef(n_estimators=50000, n_jobs=5)
clf2 = ETClassifierWithCoef(n_estimators=10000, n_jobs=3, criterion='entropy', max_features=0.85)
#gscv2 =  GridSearchCV(clf2, pars2,scoring='log_loss')

# Start training
#pca = PCA(n_components = npca)
#train[:,nfeatures+1:] = pca.fit_transform(train_fourbyte)
#del train_fourbyte
print('training started')
#clf.fit(train, y_train)
#gscv2.fit(train, y_train)
#clf2.fit(train, y_train)
#gscv3.fit(train, y_train)
#clf4.fit(train, y_train)
#gscv5.fit(train, y_train)
#print gscv2.best_score_
#print gscv2.best_params_
rfecv = RFECV(estimator=clf2, step=5, cv=StratifiedKFold(y_train, 4),
              scoring='log_loss')
rfecv.fit(train, y_train)
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score (nb of correct classifications)")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()
print('training completed')


# We don't need training set now
del train

# Dimensions for test set
ntest = 10873
max_features = 65795
ntwobyte = 256
nfeatures = 3 + ntwobyte # 259 to not use four_byte
nfourbyte = 20536
npca = 0 #500
nrows = nfeatures + npca + 5 + ntwobyte
test = np.zeros((ntest, nrows), dtype = float)
feature_array = np.zeros(ntwobyte, dtype=float)
#test_fourbyte = np.zeros((ntest, nfourbyte), dtype = int)
Ids = []    # Required test set ids

with gzip.open(ftest, read_mode) as f:
    next(f)    # Ignoring header
    for t,row in enumerate(reader(f)):
        test[t,1:nfeatures+1] = map(float, row[1:nfeatures+1]) if six.PY2 else list(map(float, row[1:nfeatures+1]))
        feature_array = test[t,4:nfeatures+1]
        test[t,nfeatures+1] = np.median(feature_array)
        test[t,nfeatures+2] = np.std(feature_array)
        test[t,nfeatures+3] = np.max(feature_array)
        test[t,nfeatures+4] = np.min(feature_array)
        sort_list = sorted(range(len(feature_array)),key=lambda x:feature_array[x], reverse=True)
        test[t,nfeatures+5:nfeatures+5+ntwobyte] = sorted(range(len(feature_array)),key=lambda x:sort_list[x])

#        test_fourbyte[t] = map(float, row[nfeatures+1:nfeatures+nfourbyte+1]) if six.PY2 else list(map(float, row[nfeatures+1:nfeatures+nfourbyte+1]))
        Ids.append(row[0])
        if(t+1)%1000==0:
            print(t+1, 'records loaded')
print('test set loaded')

# Predict for whole test set
#test[:,nfeatures+1:] = pca.transform(test_fourbyte)                                     
#del test_fourbyte
y_pred = rfecv.predict_proba(test)
#y_pred = gscv2.predict_proba(test)
#y_pred = clf2.predict_proba(test)
# Writing results to file
with gzip.open(fsubmission, write_mode) as f:
    fw = writer(f)
    # Header preparation
    header = ['Id'] + ['Prediction'+str(i) for i in range(1,10)]
    fw.writerow(header)
    for t, (Id, pred) in enumerate(zp(Ids, y_pred.tolist())):
        fw.writerow([Id]+pred)
        if(t+1)%1000==0:
            print(t+1, 'prediction written')
